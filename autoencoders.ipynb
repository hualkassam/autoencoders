{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Hussam Qassim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Autoencoders are artificial neural networks capable of learning efficient representations of the input data,\n",
    "called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have\n",
    "a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction. More \n",
    "importantly, autoencoders act as powerful feature detectors, and they can be used for unsupervised pretraining\n",
    "of deep neural networks. Lastly, they are capable of randomly generating new data that looks very similar to\n",
    "the training data; this is called a generative model. For example, you could train an autoencoder on pictures\n",
    "of faces, and it would then be able to generate new faces\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing PCA with an Undercomplete Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "If the autoencoder uses only linear activations and the cost function is the Mean Squared Error (MSE), then it \n",
    "can be shown that it ends up performing Principal Component Analysis. The following code builds a simple linear\n",
    "autoencoder to perform PCA on a 3D dataset, projecting it to 2D\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 3 # 3D inputs\n",
    "n_hidden = 2 # 2D codings\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "hidden = fully_connected(X, n_hidden, activation_fn=None)\n",
    "outputs = fully_connected(hidden, n_outputs, activation_fn=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "'''\n",
    "This code is really not very different from all the MLPs we built. The two things to note are:\n",
    "- The number of outputs is equal to the number of inputs.\n",
    "- To perform simple PCA, we set activation_fn=None (i.e., all neurons are linear) and the cost function is the\n",
    "MSE. We will see more complex autoencoders shortly.\n",
    "Now let’s load the dataset, train the model on the training set, and use it to encode the test set \n",
    "(i.e., project it to 2D)\n",
    "'''\n",
    "# Build 3D dataset\n",
    "import numpy.random as rnd\n",
    "\n",
    "rnd.seed(4)\n",
    "m = 200\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "data = np.empty((m, 3))\n",
    "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2\n",
    "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2\n",
    "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:100])\n",
    "X_test = scaler.transform(data[100:])\n",
    "\n",
    "# Train the model\n",
    "n_iterations = 1000\n",
    "codings = hidden # the output of the hidden layer provides the codings\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        training_op.run(feed_dict={X: X_train}) # no labels (unsupervised)\n",
    "        codings_val = codings.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Just like other neural networks we have discussed, autoencoders can have multiple hidden layers. In this\n",
    "case they are called stacked autoencoders (or deep autoencoders). Adding more layers helps the autoencoder \n",
    "learn more complex codings. However, one must be careful not to make the autoencoder too powerful. Imagine an\n",
    "encoder so powerful that it just learns to map each input to a single arbitrary number (and the decoder learns\n",
    "the reverse mapping). Obviously such an autoencoder will reconstruct the training data perfectly, but it will\n",
    "not have learned any useful data representation in the process (and it is unlikely to generalize well to new \n",
    "instances). The architecture of a stacked autoencoder is typically symmetrical with regards to the central \n",
    "hidden layer (the coding layer). To put it simply, it looks like a sandwich. For example, an autoencoder for \n",
    "MNIST may have 784 inputs, followed by a hidden layer with 300 neurons, then a central hidden layer of 150 \n",
    "neurons, then another hidden layer with 300 neurons, and an output layer with 784 neurons\n",
    "'''\n",
    "'''\n",
    "You can implement a stacked autoencoder very much like a regular deep MLP. For example, the following code\n",
    "builds a stacked autoencoder for MNIST, using He initialization, the ELU activation function, and l2 \n",
    "regularization. The code should look very familiar, except that there are no labels (no y)\n",
    "'''\n",
    "# Let's use MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_inputs = 28 * 28 # for MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150 # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "with tf.contrib.framework.arg_scope(\n",
    "                                [fully_connected],\n",
    "                                activation_fn=tf.nn.elu,\n",
    "                                weights_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                weights_regularizer=tf.contrib.layers.l2_regularizer(l2_reg)):\n",
    "                hidden1 = fully_connected(X, n_hidden1)\n",
    "                hidden2 = fully_connected(hidden1, n_hidden2) # codings\n",
    "                hidden3 = fully_connected(hidden2, n_hidden3)\n",
    "                outputs = fully_connected(hidden3, n_outputs, activation_fn=None)\n",
    "                \n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# You can then train the model normally. Note that the digit labels (y_batch) are unused\n",
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "When an autoencoder is neatly symmetrical, like the one we just built, a common technique is to tie the weights\n",
    "of the decoder layers to the weights of the encoder layers. This halves the number of weights in the model,\n",
    "speeding up training and limiting the risk of overfitting\n",
    "'''\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.transpose(weights2, name=\"weights3\") # tied weights\n",
    "weights4 = tf.transpose(weights1, name=\"weights4\") # tied weights\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "loss = reconstruction_loss + reg_loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "'''\n",
    "This code is fairly straightforward, but there are a few important things to note:\n",
    "- First, weight3 and weights4 are not variables, they are respectively the transpose of weights2 and weights1 \n",
    "(they are “tied” to them).\n",
    "- Second, since they are not variables, it’s no use regularizing them: we only regularize weights1 and weights2.\n",
    "- Third, biases are never tied, and never regularized.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training One Autoencoder at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Rather than training the whole stacked autoencoder in one go like we just did, it is often much faster to train\n",
    "one shallow autoencoder at a time, then stack all of them into a single stacked autoencoder (hence the name).\n",
    "During the first phase of training, the first autoencoder learns to reconstruct the inputs. During the second\n",
    "phase, the second autoencoder learns to reconstruct the output of the first autoencoder’s hidden layer.\n",
    "Finally, you just build a big sandwich using all these autoencoders,(i.e., you first stack the hidden layers\n",
    "of each autoencoder, then the output layers in reverse order). This gives you the final stacked autoencoder.\n",
    "You could easily train more autoencoders this way, building a very deep stacked autoencoder. To implement this\n",
    "multiphase training algorithm, the simplest approach is to use a different TensorFlow graph for each phase.\n",
    "After training an autoencoder, you just run the training set through it and capture the output of the hidden \n",
    "layer. This output then serves as the training set for the next autoencoder. Once all autoencoders have been\n",
    "trained this way, you simply copy the weights and biases from each autoencoder and use them to build the \n",
    "stacked autoencoder. Another approach is to use a single graph containing the whole stacked autoencoder, plus\n",
    "some extra operations to perform each training phase.\n",
    "This deserves a bit of explanation:\n",
    "- The central column in the graph is the full stacked autoencoder. This part can be used after training.\n",
    "- The left column is the set of operations needed to run the first phase of training. It creates an output\n",
    "layer that bypasses hidden layers 2 and 3. This output layer shares the same weights and biases as the stacked\n",
    "autoencoder’s output layer. On top of that are the training operations that will aim at making the output as\n",
    "close as possible to the inputs. Thus, this phase will train the weights and biases for the hidden layer 1 and\n",
    "the output layer (i.e., the first autoencoder). \n",
    "- The right column in the graph is the set of operations needed to run the second phase of training. It adds \n",
    "the training operation that will aim at making the output of hidden layer 3 as close as possible to the output\n",
    "of hidden layer 1. Note that we must freeze hidden layer 1 while running phase 2. This phase will train the \n",
    "weights and biases for hidden layers 2 and 3 (i.e., the second autoencoder).\n",
    "'''\n",
    "[...] # Build the whole stacked autoencoder normally.\n",
    "      # In this example, the weights are not tied.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "with tf.name_scope(\"phase1\"):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights4) + biases4\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1) + regularizer(weights4)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "with tf.name_scope(\"phase2\"):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2, weights3, biases3]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars)\n",
    "    \n",
    "'''\n",
    "The first phase is rather straightforward: we just create an output layer that skips hidden layers 2 and 3,\n",
    "then build the training operations to minimize the distance between the outputs and the inputs (plus some\n",
    "regularization). The second phase just adds the operations needed to minimize the distance between the output\n",
    "of hidden layer 3 and hidden layer 1 (also with some regularization). Most importantly, we provide the list of\n",
    "trainable variables to the minimize() method, making sure to leave out weights1 and\t biases1 ; this effectively\n",
    "freezes hidden layer 1 during phase 2. During the execution phase, all you need to do is run the phase 1 \n",
    "training op for a number of epochs, then the phase 2 training op for some more epochs.\n",
    "\n",
    "TIP: \n",
    "Since hidden layer 1 is frozen during phase 2, its output will always be the same for any given training \n",
    "instance. To avoid having to recompute the output of hidden layer 1 at every single epoch, you can compute it \n",
    "for the whole training set at the end of phase 1, then directly feed the cached output of hidden layer 1 \n",
    "during phase 2. This can give you a nice performance boost.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs. They must\n",
    "be fairly similar, and the differences should be unimportant details. Let’s plot two random digits and their\n",
    "reconstructions\n",
    "'''\n",
    "n_test_digits = 2\n",
    "X_test = mnist.test.images[:n_test_digits]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    [...] # Train the Autoencoder\n",
    "    outputs_val\t=\toutputs.eval(feed_dict={X:\tX_test})\n",
    "    \n",
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "for digit_index in range(n_test_digits):\n",
    "    plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "    plot_image(X_test[digit_index])\n",
    "    plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "    plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Once your autoencoder has learned some features, you may want to take a look at them. There are various \n",
    "techniques for this. Arguably the simplest technique is to consider each neuron in every hidden layer, and\n",
    "find the training instances that activate it the most. This is especially useful for the top hidden layers since\n",
    "they often capture relatively large features that you can easily spot in a group of training instances that\n",
    "contain them. For example, if a neuron strongly activates when it sees a cat in a picture, it will be pretty\n",
    "obvious that the pictures that activate it the most all contain cats. However, for lower layers, this technique\n",
    "does not work so well, as the features are smaller and more abstract, so it’s often hard to understand exactly\n",
    "what the neuron is getting all excited about. Let’s look at another technique. For each neuron in the first \n",
    "hidden layer, you can create an image where a pixel’s intensity corresponds to the weight of the connection to\n",
    "the given neuron. For example, the following code plots the features learned by five neurons in the first \n",
    "hidden layer\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    [...] # train autoencoder\n",
    "    weights1_val = weights1.eval()\n",
    "    \n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_image(weights1_val.T[i])\n",
    "    \n",
    "'''\n",
    "The first four features seem to correspond to small patches, while the fifth feature seems to look for vertical\n",
    "strokes (note that these features come from the stacked denoising autoencoder that we will discuss later).\n",
    "Another technique is to feed the autoencoder a random input image, measure the activation of the neuron you are\n",
    "interested in, and then perform backpropagation to tweak the image in such a way that the neuron will activate\n",
    "even more. If you iterate several times (performing gradient ascent), the image will gradually turn into the\n",
    "most exciting image (for the neuron). This is a useful technique to visualize the kinds of inputs that a neuron\n",
    "is looking for. Finally, if you are using an autoencoder to perform unsupervised pretraining — for example, for\n",
    "a classification task — a simple way to verify that the features learned by the autoencoder are useful is to\n",
    "measure the performance of the classifier\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Pretraining Using Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "If you are tackling a complex supervised task but you do not have a lot of labeled training data, one solution\n",
    "is to find a neural network that performs a similar task, and then reuse its lower layers. This makes it \n",
    "possible to train a high-performance model using only little training data because your neural network won’t\n",
    "have to learn all the low-level features; it will just reuse the feature detectors learned by the existing net.\n",
    "Similarly, if you have a large dataset but most of it is unlabeled, you can first train a stacked autoencoder\n",
    "using all the data, then reuse the lower layers to create a neural network for your actual task, and train it\n",
    "using the labeled data. The stacked autoencoder itself is typically trained one autoencoder at a time, as \n",
    "discussed earlier. When training the classifier, if you really don’t have much labeled training data, you may\n",
    "want to freeze the pretrained layers (at least the lower ones).There is nothing special about the TensorFlow\n",
    "implementation: just train an autoencoder using all the training data, then reuse its encoder layers to create\n",
    "a new neural network\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Another way to force the autoencoder to learn useful features is to add noise to its inputs, training it to\n",
    "recover the original, noise-free inputs. This prevents the autoencoder from trivially copying its inputs to\n",
    "its outputs, so it ends up having to find patterns in the data. The idea of using autoencoders to remove noise\n",
    "has been around since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008 paper,\n",
    "Pascal Vincent et al. showed that autoencoders could also be used for feature extraction. In a 2010 paper, \n",
    "Vincent et al. introduced stacked denoising autoencoders. The noise can be pure Gaussian noise added to the\n",
    "inputs, or it can be randomly switched off inputs, just like in dropout.\n",
    "'''\n",
    "'''\n",
    "Implementing denoising autoencoders in TensorFlow is not too hard. Let’s start with Gaussian noise. It’s really\n",
    "just like training a regular autoencoder, except you add noise to the inputs, and the reconstruction loss is\n",
    "calculated based on the original inputs\n",
    "'''\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_noisy = X + tf.random_normal(tf.shape(X))\n",
    "[...]\n",
    "hidden1 = activation(tf.matmul(X_noisy, weights1) + biases1)\n",
    "[...]\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "[...]\n",
    "\n",
    "'''\n",
    "Since the shape of X is only partially defined during the construction phase, we cannot know in advance the \n",
    "shape of the noise that we must add to X. We cannot call X.get_shape() because this would just return the \n",
    "partially defined shape of X ( [None, n_inputs] ), and random_normal() expects a fully defined shape so it \n",
    "would raise an exception. Instead, we call tf.shape(X), which creates an operation that will return the shape\n",
    "of X at runtime, which will be fully defined at that point.\n",
    "'''\n",
    "# Implementing the dropout version, which is more common, is not much harder\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "keep_prob = 0.7\n",
    "is_training = tf.placeholder_with_default(False, shape=(), name='is_training')\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_drop = dropout(X, keep_prob, is_training=is_training)\n",
    "[...]\n",
    "hidden1 = activation(tf.matmul(X_drop, weights1) + biases1)\n",
    "[...]\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "[...]\n",
    "\n",
    "'''\n",
    "During training we must set is_training to True using the feed_dict. However, during testing it is not \n",
    "necessary to set is_training to False, since we set that as the default in the call to the \n",
    "placeholder_with_default() function\n",
    "'''\n",
    "sess.run(training_op, feed_dict={X: X_batch, is_training: True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Another kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate\n",
    "term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer.\n",
    "For example, it may be pushed to have on average only 5% significantly active neurons in the coding layer. \n",
    "This forces the autoencoder to represent each input as a combination of a small number of activations. As a \n",
    "result, each neuron in the coding layer typically ends up representing a useful feature (if you could speak \n",
    "only a few words per month, you would probably try to make them worth listening to). In order to favor sparse\n",
    "models, we must first measure the actual sparsity of the coding layer at each training iteration. We do so by\n",
    "computing the average activation of each neuron in the coding layer, over the whole training batch. The batch\n",
    "size must not be too small, or else the mean will not be accurate. Once we have the mean activation per neuron,\n",
    "we want to penalize the neurons that are too active by adding a sparsity loss to the cost function. For example,\n",
    "if we measure that a neuron has an average activation of 0.3, but the target sparsity is 0.1, it must be \n",
    "penalized to activate less. One approach could be simply adding the squared error (0.3 – 0.1)2 to the cost \n",
    "function, but in practice a better approach is to use the Kullback–Leibler divergence, which has much stronger\n",
    "gradients than the Mean Squared Error\n",
    "'''\n",
    "def kl_divergence(p, q):\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "learning_rate = 0.01\n",
    "sparsity_target = 0.1\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "[...] # Build a normal autoencoder (in this example the coding layer is hidden1)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0) # batch mean\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "'''\n",
    "An important detail is the fact that the activations of the coding layer must be between 0 and 1 (but not\n",
    "equal to 0 or 1), or else the KL divergence will return NaN (Not a Number). A simple solution is to use the \n",
    "logistic activation function for the coding layer\n",
    "'''\n",
    "hidden1 = tf.nn.sigmoid(tf.matmul(X, weights1) + biases1)\n",
    "\n",
    "'''\n",
    "One simple trick can speed up convergence: instead of using the MSE, we can choose a reconstruction loss that\n",
    "will have larger gradients. Cross entropy is often a good choice. To use it, we must normalize the inputs to\n",
    "make them take on values from 0 to 1, and use the logistic activation function in the output layer so the \n",
    "outputs also take on values from 0 to 1. TensorFlow’s sigmoid_cross_entropy_with_logits() function takes care\n",
    "of efficiently applying the logistic (sigmoid) activation function to the outputs and computing the cross \n",
    "entropy. Note that the outputs operation is not needed during training (we use it only when we want to look \n",
    "at the reconstructions)\n",
    "'''\n",
    "[...]\n",
    "logits = tf.matmul(hidden1, weights2) + biases2)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "reconstruction_loss = tf.reduce_sum(\n",
    "                        tf.nn.sigmoid_cross_entropy_with_logits(labels=X,\tlogits=logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Another important category of autoencoders was introduced in 2014 by Diederik Kingma and Max Welling, and has \n",
    "quickly become one of the most popular types of autoencoders: variational autoencoders. They are quite \n",
    "different from all the autoencoders we have discussed so far, in particular:\n",
    "- They are probabilistic autoencoders, meaning that their outputs are partly determined by chance, even after\n",
    "training (as opposed to denoising autoencoders, which use randomness only during training).\n",
    "- Most importantly, they are generative autoencoders, meaning that they can generate new instances that look\n",
    "like they were sampled from the training set.\n",
    "Both these properties make them rather similar to RBMs, but they are easier to train and the sampling process\n",
    "is much faster (with RBMs you need to wait for the network to stabilize into a “thermal equilibrium” before\n",
    "you can sample a new instance). Let’s take a look at how they work. You can recognize, of course, the basic\n",
    "structure of all autoencoders, with an encoder followed by a decoder (in this example, they both have two \n",
    "hidden layers), but there is a twist: instead of directly producing a coding for a given input, the encoder\n",
    "produces a mean coding μ and a standard deviation σ. The actual coding is then sampled randomly from a Gaussian\n",
    "distribution with mean μ and standard deviation σ. After that the decoder just decodes the sampled coding \n",
    "normally. The right part of the diagram shows a training instance going through this autoencoder.First, the\n",
    "encoder produces μ and σ, then a coding is sampled randomly (notice that it is not exactly located at μ), and\n",
    "finally this coding is decoded, and the final output resembles the training instance. although the inputs may\n",
    "have a very convoluted distribution, a variational autoencoder tends to produce codings that look as though  \n",
    "they were sampled from a simple Gaussian distribution: during training, the cost function (discussed next) \n",
    "pushes the codings to gradually migrate within the coding space (also called the latent space) to occupy a \n",
    "roughly (hyper)spherical region that looks like a cloud of Gaussian points. One great consequence is that \n",
    "after training a variational autoencoder, you can very easily generate a new instance: just sample a random\n",
    "coding from the Gaussian distribution, decode it, and voilà! So let’s look at the cost function. It is composed\n",
    "of two parts. The first is the usual reconstruction loss that pushes the autoencoder to reproduce its inputs \n",
    "(we can use cross entropy for this, as discussed earlier). The second is the latent loss that pushes the \n",
    "autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution, for \n",
    "which we use the KL divergence between the target distribution (the Gaussian distribution) and the actual \n",
    "distribution of the codings. The math is a bit more complex than earlier, in particular because of the \n",
    "Gaussian noise, which limits the amount of information that can be transmitted to the coding layer (thus \n",
    "pushing the autoencoder to learn useful features). Luckily, the equations simplify to the following code for\n",
    "the latent loss\n",
    "'''\n",
    "eps = 1e-10 # smoothing term to avoid computing log(0) which is NaN\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "                            tf.square(hidden3_sigma)\t+\ttf.square(hidden3_mean)\n",
    "                            - 1 - tf.log(eps + tf.square(hidden3_sigma)))\n",
    "\n",
    "'''\n",
    "One common variant is to train the encoder to output γ = log(σ 2 ) rather than σ. Wherever we need σ we\n",
    "can just compute ( σ = exp(γ/2)). This makes it a bit easier for the encoder to capture sigmas of different\n",
    "scales, and thus it helps speed up convergence. The latent loss ends up a bit simpler\n",
    "'''\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "                tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)\n",
    "\n",
    "# The following code builds the variational autoencoder, using the log(σ2) variant\n",
    "n_inputs = 28 * 28 # for MNIST\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20 # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.001\n",
    "with tf.contrib.framework.arg_scope(\n",
    "                                [fully_connected],\n",
    "                                activation_fn=tf.nn.elu,\n",
    "                                weights_initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "    hidden1 = fully_connected(X, n_hidden1)\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2)\n",
    "    hidden3_mean = fully_connected(hidden2, n_hidden3, activation_fn=None)\n",
    "    hidden3_gamma = fully_connected(hidden2, n_hidden3, activation_fn=None)\n",
    "    hidden3_sigma = tf.exp(0.5 * hidden3_gamma)\n",
    "    noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "    hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "    hidden4 = fully_connected(hidden3, n_hidden4)\n",
    "    hidden5 = fully_connected(hidden4, n_hidden5)\n",
    "    logits = fully_connected(hidden5, n_outputs, activation_fn=None)\n",
    "    outputs = tf.sigmoid(logits)\n",
    "\n",
    "reconstruction_loss = tf.reduce_sum(\n",
    "                            tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits))\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "                            tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)\n",
    "cost = reconstruction_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now let’s use this variational autoencoder to generate images that look like handwritten digits. All we need to\n",
    "do is train the model, then sample random codings from a Gaussian distribution and decode them\n",
    "'''\n",
    "import numpy as np\n",
    "n_digits = 60\n",
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "    codings_rnd = np.random.normal(size=[n_digits, n_hidden3])\n",
    "    outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})\n",
    "    \n",
    "# Now we can see what the “handwritten” digits produced by the autoencoder look like\n",
    "for iteration in range(n_digits):\n",
    "    plt.subplot(n_digits, 10, iteration + 1)\n",
    "    plot_image(outputs_val[iteration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Contractive autoencoder (CAE): The autoencoder is constrained during training so that the derivatives of the\n",
    "codings with regards to the inputs are small. In other words, two similar inputs must have similar codings.\n",
    "Stacked convolutional autoencoders: Autoencoders that learn to extract visual features by reconstructing \n",
    "images processed through convolutional layers.\n",
    "Generative stochastic network (GSN): A generalization of denoising autoencoders, with the added capability to\n",
    "generate data.\n",
    "Winner-take-all (WTA) autoencoder: During training, after computing the activations of all the neurons in the\n",
    "coding layer, only the top k% activations for each neuron over the training batch are preserved, and the rest\n",
    "are set to zero. Naturally this leads to sparse codings. Moreover, a similar WTA approach can be used to \n",
    "produce sparse convolutional autoencoders.\n",
    "Adversarial autoencoders: One network is trained to reproduce its inputs, and at the same time another is \n",
    "trained to find inputs that the first network is unable to properly reconstruct. This pushes the first \n",
    "autoencoder to learn robust codings.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
